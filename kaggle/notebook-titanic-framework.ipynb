{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-21T04:54:32.098251Z","iopub.execute_input":"2022-01-21T04:54:32.09867Z","iopub.status.idle":"2022-01-21T04:54:32.134698Z","shell.execute_reply.started":"2022-01-21T04:54:32.098565Z","shell.execute_reply":"2022-01-21T04:54:32.133787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pandas as pd\n# from pandas.tools.plotting import scatter_matrix\n\n#Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:32.13648Z","iopub.execute_input":"2022-01-21T04:54:32.136799Z","iopub.status.idle":"2022-01-21T04:54:33.689423Z","shell.execute_reply.started":"2022-01-21T04:54:32.136763Z","shell.execute_reply":"2022-01-21T04:54:33.688488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import dataset","metadata":{}},{"cell_type":"code","source":"data_raw = pd.read_csv('../input/titanic/train.csv')\ndata_val  = pd.read_csv('../input/titanic/test.csv')\n\ndata1 = data_raw.copy(deep = True)\n\ndata_cleaner = [data1, data_val]\n\nprint (data_raw.info())\n\ndata_raw.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:33.690895Z","iopub.execute_input":"2022-01-21T04:54:33.691246Z","iopub.status.idle":"2022-01-21T04:54:33.765514Z","shell.execute_reply.started":"2022-01-21T04:54:33.691202Z","shell.execute_reply":"2022-01-21T04:54:33.764585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cleaning: correcting, completing, creating and converting","metadata":{}},{"cell_type":"code","source":"# find missing data\n\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\n\ndata_raw.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:33.767256Z","iopub.execute_input":"2022-01-21T04:54:33.767504Z","iopub.status.idle":"2022-01-21T04:54:33.818166Z","shell.execute_reply.started":"2022-01-21T04:54:33.767475Z","shell.execute_reply":"2022-01-21T04:54:33.817386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill in missing values\nfor dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n# drop columns\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(data_val.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:33.819591Z","iopub.execute_input":"2022-01-21T04:54:33.8198Z","iopub.status.idle":"2022-01-21T04:54:33.836449Z","shell.execute_reply.started":"2022-01-21T04:54:33.819774Z","shell.execute_reply":"2022-01-21T04:54:33.835569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature engineering\nfor dataset in data_cleaner:    \n\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 \n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 \n\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\nstat_min = 10 \ntitle_names = (data1['Title'].value_counts() < stat_min) \n\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\n\n\ndata1.info()\ndata_val.info()\ndata1.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:33.837741Z","iopub.execute_input":"2022-01-21T04:54:33.837948Z","iopub.status.idle":"2022-01-21T04:54:33.941154Z","shell.execute_reply.started":"2022-01-21T04:54:33.837922Z","shell.execute_reply":"2022-01-21T04:54:33.940292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encode categorical data.","metadata":{}},{"cell_type":"code","source":"# label encoding\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n# Y \nTarget = ['Survived']\n\n\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\n\ndata1_dummy.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:33.942669Z","iopub.execute_input":"2022-01-21T04:54:33.94305Z","iopub.status.idle":"2022-01-21T04:54:33.999904Z","shell.execute_reply.started":"2022-01-21T04:54:33.943007Z","shell.execute_reply":"2022-01-21T04:54:33.99922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split train and test data.","metadata":{}},{"cell_type":"code","source":"train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:34.001173Z","iopub.execute_input":"2022-01-21T04:54:34.001623Z","iopub.status.idle":"2022-01-21T04:54:34.027432Z","shell.execute_reply.started":"2022-01-21T04:54:34.001584Z","shell.execute_reply":"2022-01-21T04:54:34.026729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"exploratory data analysis","metadata":{}},{"cell_type":"code","source":"# Discrete Variable Correlation by Survival using\n# group by aka pivot table\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n\n# crosstabs\nprint(pd.crosstab(data1['Title'],data1[Target[0]]))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:34.029587Z","iopub.execute_input":"2022-01-21T04:54:34.029921Z","iopub.status.idle":"2022-01-21T04:54:34.098979Z","shell.execute_reply.started":"2022-01-21T04:54:34.029892Z","shell.execute_reply":"2022-01-21T04:54:34.098395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=[16,12])\n\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:34.099874Z","iopub.execute_input":"2022-01-21T04:54:34.100627Z","iopub.status.idle":"2022-01-21T04:54:35.691197Z","shell.execute_reply.started":"2022-01-21T04:54:34.100593Z","shell.execute_reply":"2022-01-21T04:54:35.690238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use seaborn graphics for multi-variable comparison\n\n#graph individual features by survival\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:35.692846Z","iopub.execute_input":"2022-01-21T04:54:35.693522Z","iopub.status.idle":"2022-01-21T04:54:37.526867Z","shell.execute_reply.started":"2022-01-21T04:54:35.693482Z","shell.execute_reply":"2022-01-21T04:54:37.526147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(16, 8))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:37.528003Z","iopub.execute_input":"2022-01-21T04:54:37.528357Z","iopub.status.idle":"2022-01-21T04:54:38.427197Z","shell.execute_reply.started":"2022-01-21T04:54:37.528296Z","shell.execute_reply":"2022-01-21T04:54:38.426585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# graph distribution of qualitative data: Sex\nfig, qaxis = plt.subplots(1,3,figsize=(16,8))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:38.428275Z","iopub.execute_input":"2022-01-21T04:54:38.4286Z","iopub.status.idle":"2022-01-21T04:54:39.552323Z","shell.execute_reply.started":"2022-01-21T04:54:38.428572Z","shell.execute_reply":"2022-01-21T04:54:39.551514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(16,8))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:39.553584Z","iopub.execute_input":"2022-01-21T04:54:39.553825Z","iopub.status.idle":"2022-01-21T04:54:40.912905Z","shell.execute_reply.started":"2022-01-21T04:54:39.553795Z","shell.execute_reply":"2022-01-21T04:54:40.911961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how does embark port factor with class, sex, and survival compare\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:40.914285Z","iopub.execute_input":"2022-01-21T04:54:40.914582Z","iopub.status.idle":"2022-01-21T04:54:42.448475Z","shell.execute_reply.started":"2022-01-21T04:54:40.914548Z","shell.execute_reply":"2022-01-21T04:54:42.447657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:42.450181Z","iopub.execute_input":"2022-01-21T04:54:42.450497Z","iopub.status.idle":"2022-01-21T04:54:43.03Z","shell.execute_reply.started":"2022-01-21T04:54:42.450454Z","shell.execute_reply":"2022-01-21T04:54:43.029247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:43.03138Z","iopub.execute_input":"2022-01-21T04:54:43.031606Z","iopub.status.idle":"2022-01-21T04:54:45.172763Z","shell.execute_reply.started":"2022-01-21T04:54:43.031578Z","shell.execute_reply":"2022-01-21T04:54:45.171782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:54:45.174162Z","iopub.execute_input":"2022-01-21T04:54:45.174523Z","iopub.status.idle":"2022-01-21T04:55:26.985051Z","shell.execute_reply.started":"2022-01-21T04:54:45.174492Z","shell.execute_reply":"2022-01-21T04:55:26.984092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:55:26.986395Z","iopub.execute_input":"2022-01-21T04:55:26.986633Z","iopub.status.idle":"2022-01-21T04:55:28.249134Z","shell.execute_reply.started":"2022-01-21T04:55:26.986602Z","shell.execute_reply":"2022-01-21T04:55:28.248291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Modeling","metadata":{}},{"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T04:58:05.273415Z","iopub.execute_input":"2022-01-21T04:58:05.273714Z","iopub.status.idle":"2022-01-21T04:58:24.14828Z","shell.execute_reply.started":"2022-01-21T04:58:05.273684Z","shell.execute_reply":"2022-01-21T04:58:24.147376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:00:31.968771Z","iopub.execute_input":"2022-01-21T05:00:31.969752Z","iopub.status.idle":"2022-01-21T05:00:32.503083Z","shell.execute_reply.started":"2022-01-21T05:00:31.969707Z","shell.execute_reply":"2022-01-21T05:00:32.502224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tune model with hyperparameters - grid search and cross validation\n","metadata":{}},{"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n             }\n\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:20:39.733133Z","iopub.execute_input":"2022-01-21T05:20:39.734143Z","iopub.status.idle":"2022-01-21T05:20:41.201156Z","shell.execute_reply.started":"2022-01-21T05:20:39.734097Z","shell.execute_reply":"2022-01-21T05:20:41.200293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tune model with feature selection - recursive feature elimination with cross validation.","metadata":{}},{"cell_type":"code","source":"#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split, return_train_score = True)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n#tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split, return_train_score = True)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(rfe_tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:27:56.710187Z","iopub.execute_input":"2022-01-21T05:27:56.710551Z","iopub.status.idle":"2022-01-21T05:27:58.293756Z","shell.execute_reply.started":"2022-01-21T05:27:56.710515Z","shell.execute_reply":"2022-01-21T05:27:58.29291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n\n# import graphviz \n# dot_data = tree.export_graphviz(dtree, out_file=None, \n#                                 feature_names = data1_x_bin, class_names = True,\n#                                 filled = True, rounded = True)\n# graph = graphviz.Source(dot_data) \n# graph","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:29:47.577763Z","iopub.execute_input":"2022-01-21T05:29:47.578042Z","iopub.status.idle":"2022-01-21T05:29:47.582421Z","shell.execute_reply.started":"2022-01-21T05:29:47.578013Z","shell.execute_reply":"2022-01-21T05:29:47.581499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"validate and implement","metadata":{}},{"cell_type":"code","source":"correlation_heatmap(MLA_predict)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:30:10.92979Z","iopub.execute_input":"2022-01-21T05:30:10.930181Z","iopub.status.idle":"2022-01-21T05:30:14.037127Z","shell.execute_reply.started":"2022-01-21T05:30:10.930137Z","shell.execute_reply":"2022-01-21T05:30:14.036228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"voting classifier","metadata":{}},{"cell_type":"code","source":"#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http://scikit-learn.org/stable/modules/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:33:32.219237Z","iopub.execute_input":"2022-01-21T05:33:32.219972Z","iopub.status.idle":"2022-01-21T05:34:07.514865Z","shell.execute_reply.started":"2022-01-21T05:33:32.219921Z","shell.execute_reply":"2022-01-21T05:34:07.5136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#IMPORTANT: THIS SECTION IS UNDER CONSTRUCTION!!!! 12.24.17\n#UPDATE: This section was scrapped for the next section; as it's more computational friendly.\n\n#WARNING: Running is very computational intensive and time expensive\n#code is written for experimental/developmental purposes and not production ready\n\n\n#tune each estimator before creating a super model\n#http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [50,100,300]\ngrid_ratio = [.1,.25,.5,.75,1.0]\ngrid_learn = [.01,.03,.05,.1,.25]\ngrid_max_depth = [2,4,6,None]\ngrid_min_samples = [5,10,.03,.05,.10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\nvote_param = [{\n#            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n            'ada__n_estimators': grid_n_estimator,\n            'ada__learning_rate': grid_ratio,\n            'ada__algorithm': ['SAMME', 'SAMME.R'],\n            'ada__random_state': grid_seed,\n    \n            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'bc__n_estimators': grid_n_estimator,\n            'bc__max_samples': grid_ratio,\n            'bc__oob_score': grid_bool, \n            'bc__random_state': grid_seed,\n            \n            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'etc__n_estimators': grid_n_estimator,\n            'etc__criterion': grid_criterion,\n            'etc__max_depth': grid_max_depth,\n            'etc__random_state': grid_seed,\n\n\n            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            'gbc__loss': ['deviance', 'exponential'],\n            'gbc__learning_rate': grid_ratio,\n            'gbc__n_estimators': grid_n_estimator,\n            'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n            'gbc__max_depth': grid_max_depth,\n            'gbc__min_samples_split': grid_min_samples,\n            'gbc__min_samples_leaf': grid_min_samples,      \n            'gbc__random_state': grid_seed,\n    \n            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'rfc__n_estimators': grid_n_estimator,\n            'rfc__criterion': grid_criterion,\n            'rfc__max_depth': grid_max_depth,\n            'rfc__min_samples_split': grid_min_samples,\n            'rfc__min_samples_leaf': grid_min_samples,   \n            'rfc__bootstrap': grid_bool,\n            'rfc__oob_score': grid_bool, \n            'rfc__random_state': grid_seed,\n        \n            #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'lr__fit_intercept': grid_bool,\n            'lr__penalty': ['l1','l2'],\n            'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n            'lr__random_state': grid_seed,\n            \n            #http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'bnb__alpha': grid_ratio,\n            'bnb__prior': grid_bool,\n            'bnb__random_state': grid_seed,\n    \n            #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'knn__n_neighbors': [1,2,3,4,5,6,7],\n            'knn__weights': ['uniform', 'distance'],\n            'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'knn__random_state': grid_seed,\n            \n            #http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n            'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'svc__C': grid_max_depth,\n            'svc__gamma': grid_ratio,\n            'svc__decision_function_shape': ['ovo', 'ovr'],\n            'svc__probability': [True],\n            'svc__random_state': grid_seed,\n    \n    \n            #http://xgboost.readthedocs.io/en/latest/parameter.html\n            'xgb__learning_rate': grid_ratio,\n            'xgb__max_depth': [2,4,6,8,10],\n            'xgb__tree_method': ['exact', 'approx', 'hist'],\n            'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n            'xgb__seed': grid_seed    \n\n        }]\n\n\n\n\n#Soft Vote with tuned models\n#grid_soft = model_selection.GridSearchCV(estimator = vote_soft, param_grid = vote_param, cv = 2, scoring = 'roc_auc')\n#grid_soft.fit(data1[data1_x_bin], data1[Target])\n\n#print(grid_soft.cv_results_.keys())\n#print(grid_soft.cv_results_['params'])\n#print('Soft Vote Tuned Parameters: ', grid_soft.best_params_)\n#print(grid_soft.cv_results_['mean_train_score'])\n#print(\"Soft Vote Tuned Training w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(grid_soft.cv_results_['mean_test_score'])\n#print(\"Soft Vote Tuned Test w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n#print(\"Soft Vote Tuned Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n#print('-'*10)\n\n\n#credit: https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/\n#cv_keys = ('mean_test_score', 'std_test_score', 'params')\n#for r, _ in enumerate(grid_soft.cv_results_['mean_test_score']):\n#    print(\"%0.3f +/- %0.2f %r\"\n#          % (grid_soft.cv_results_[cv_keys[0]][r],\n#             grid_soft.cv_results_[cv_keys[1]][r] / 2.0,\n#             grid_soft.cv_results_[cv_keys[2]][r]))\n\n\n#print('-'*10)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:36:11.583401Z","iopub.execute_input":"2022-01-21T05:36:11.583701Z","iopub.status.idle":"2022-01-21T05:36:11.601444Z","shell.execute_reply.started":"2022-01-21T05:36:11.583665Z","shell.execute_reply":"2022-01-21T05:36:11.600656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#WARNING: Running is very computational intensive and time expensive.\n#Code is written for experimental/developmental purposes and not production ready!\n\n\n#Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=’deviance’\n            'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=”gini”\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = ‘uniform’\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n            #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n            #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\n\nimport time\nstart_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): \n\n    #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n    #print(param)\n    \n    \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc', return_train_score = True)\n    best_search.fit(data1[data1_x_bin], data1[Target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T05:38:38.333267Z","iopub.execute_input":"2022-01-21T05:38:38.333604Z","iopub.status.idle":"2022-01-21T05:53:29.190635Z","shell.execute_reply.started":"2022-01-21T05:38:38.333566Z","shell.execute_reply":"2022-01-21T05:53:29.189981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hard Vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\ngrid_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score = True)\ngrid_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#12/31/17 tuned with data1_x_bin\n#The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n#The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n#The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n#The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n#The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n#The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n#The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n#The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n#The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n#The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n#The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n#The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n#Total optimization time was 5.56 minutes.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"submission\n","metadata":{}},{"cell_type":"code","source":"#prepare data for modeling\nprint(data_val.info())\nprint(\"-\"*10)\n\n#handmade decision tree - submission score = 0.77990\ndata_val['Survived'] = mytree(data_val).astype(int)\n\n\n#decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n#submit_dt = tree.DecisionTreeClassifier()\n#submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n#submit_dt.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n#data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n\n\n#bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n#submit_bc = ensemble.BaggingClassifier()\n#submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_bc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n#data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n\n\n#extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n#submit_etc = ensemble.ExtraTreesClassifier()\n#submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_etc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n#data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n\n\n#random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n#submit_rfc = ensemble.RandomForestClassifier()\n#submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_rfc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n#data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n\n\n\n#ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n#submit_abc = ensemble.AdaBoostClassifier()\n#submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_abc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n#data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n\n\n#gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n#submit_gbc = ensemble.GradientBoostingClassifier()\n#submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_gbc.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n#data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n\n#extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n#submit_xgb = XGBClassifier()\n#submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n#submit_xgb.fit(data1[data1_x_bin], data1[Target])\n#print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n#data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n\n\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n#data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n\n\n#soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n#data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n#data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\n\n\n#submit file\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"../working/submit.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\nsubmit.sample(10)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optimizate and strategize","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}